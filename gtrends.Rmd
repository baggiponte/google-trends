---
title: "R Notebook"
output: html_notebook
---

# Load Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
```

```{r libraries, warning=FALSE}
# for data retrieval
library(gtrendsR)

# essential for data manipulation
library(tidyverse)

# render markdown in plots
library(ggtext)

# arrange plot distributions
library(patchwork)

# workflow management
library(here)
here::i_am("gtrends.Rmd")

# fonts
library(showtext)

font_add_google("Yanone Kaffeesatz", # name of the Google Font
                "Yanone Kaffeesatz") # name that will be used in R
font_add_google("Fira Sans", "Fira Sans")

showtext_auto()

theme_set(theme_minimal(base_family = "Fira Sans"))
```

# Test `{gtrendsR}`

By default, the function retrieves data for the past 5 years. We can specify to retrieve the last 3 months:

```{r}
test_draghi <- gtrends("Mario Draghi", time='today 3-m')$interest_over_time %>%
  as_tibble()

test_draghi
```

## Filter what we need

```{r}
test_draghi %>% 
  select(1:3) %>% 
  tail(1) %>% pull(hits) -> num_hits

test_draghi %>% 
  select(1:3) %>% 
  tail(1) %>% pull(date) -> date_last
```


Also by using a time span formatted as `%Y-%m-%d %Y-%m-%d`. The largest interval for retrieving daily data is 9 months.

## Find out the max window size

```{r}
gtrends("Mario Draghi", time='2020-12-02 2021-08-28')$interest_over_time %>%
  as_tibble()
```

This is the largest window for which I can retrieve daily data. Let's see its length:

```{r}
test_start <- lubridate::ymd('2020-12-02')
test_end <- lubridate::ymd('2021-08-28')

test_end - test_start
```

This is the largest window we can use to retrieve the data.

## Write a Function to Slide Windows by Nine Months

We need a function that works like this:

* Takes in two strings, representing the start and the end date of the timespan of interest
* Takes the start date and adds 9 months
* Returns a timespan (string format)

```{r}
# input dates
start <- "2012-01-01"
end <- "2021-08-28"

# parse them as datetime objects - needed for adding days
start_date <- start %>% lubridate::ymd()
end_date <- end %>% lubridate::ymd()

# create window for sliding
window <- lubridate::days(269)

# create chunks
chunk_start <- start_date
chunk_end <- start_date + window

make_timespan <- function(start_date, end_date) {
  glue::glue("{start_date} {end_date}")
}

first_span <- make_timespan(chunk_start, chunk_end)
```

Then we use this to retrieve the data:

```{r}
first_fetch <- gtrends("Mario Draghi", time=first_span)$interest_over_time %>%
  as_tibble()

first_fetch %>% head()
```

Now, what we want to do is to retrieve the row where the value of hits is maximum:

```{r}
first_fetch %>% 
  filter(
    hits == max(hits)
  )
```

And extract the date:

```{r}
new_start_date <- first_fetch %>% 
  filter(
    hits == max(hits)
  ) %>% 
  pull(date) %>% lubridate::ymd()

new_start_date
```

This date becomes the new starting point for another data retrieval:

```{r}
new_timespan <- make_timespan(new_start_date, new_start_date + window)

new_fetch <- gtrends("Mario Draghi", time=new_timespan)$interest_over_time %>% as_tibble()

new_fetch %>% head()
```

Now we need to scale the values on `2012-09-06` and scale the newly fetched series:

```{r}
ratio <- new_fetch[[1,2]] / 100

new_fetch %>% 
  mutate(
    hits = (hits / ratio) %>% round()
  )
```

And now we are ready to reiterate the procedure:

```{r}
new_fetch %>% 
  mutate(
    hits = (hits / ratio) %>% round()
  ) %>% 
  filter(
    hits == max(hits)
  )
```

# Turn the procedure into a series of functions

The iterative procedure is:

1. Retrieve the first chunk.
  * Turn `hits` into numeric integers from 1 to 100.
2. Find the date of the local chunk maximum. This will be the new chunk starting date.
3. Create a new timespan from the new starting date.
3. Retrieve the largest subsequent chunk.
4. Compute the ratio to scale the newer chunk with respect to the older one.
5. Scale the newer chunk.
6. Append the newest rows.
7. Scale the whole series back to the range 1-100.
8. Go back to 2.

```{r}
# copied here for having everything in one place
make_timespan <- function(start_date, end_date) {
  glue::glue("{start_date} {end_date}")
}

retrieve_data <- function(keyword, timespan, loop_number) {
  
  gtrends(keyword, time = timespan)$interest_over_time %>%
      as_tibble() %>% 
      # only select columns we need
      select(1:3) %>% 
      mutate(
        # keep track of the fetch
        loop = loop_number,
        hits = case_when(
          # if character appears, turn into 0
          hits == "<1" ~ 0,
          # add 1 to everything to avoid zeros
          TRUE ~ hits + 1
        ))
}

fetch_local_max_date <- function(older_chunk) {
  older_chunk %>% 
    filter(
      hits == max(hits)
    ) %>% 
    pull(date) %>% lubridate::ymd()
}

fetch_last_date <- function(older_chunk) {
  older_chunk %>% 
    tail(1) %>% 
    pull(date) %>% lubridate::ymd()
}

scale_series <- function(newer_chunk, older_chunk) {
  
  # compute the ratio used to scale the newest series
  ratio <- newer_chunk[[1,2]] / 100
  
  newer_chunk %>% 
    mutate(hits = (hits / ratio) %>% round())
}

concatenate_series <- function(older_chunk, newer_chunk) {
  
  # define the split date to drop the observation of the older chunk
  split_date <- fetch_local_max_date(older_chunk)
  
  older_chunk %>% 
    # keep dates before the split date
    filter(date < split_date) %>% 
    # append newest rows
    bind_rows(newer_chunk) %>% 
    # scale the whole series
    mutate(hits = (hits / max(hits) * 100) %>% round())
}

retrieve_daily_data <- function(start, end, keyword, verbose = FALSE) {
  
  # parse them as datetime objects - needed for adding days
  start_date <- start %>% lubridate::ymd()
  end_date <- end %>% lubridate::ymd()
  
  # create window for sliding
  window <- lubridate::days(269)
  
  # create chunk starting and ending dates
  chunk_start <- start_date
  chunk_end <- start_date + window
  
  # define span
  chunk_span <- make_timespan(chunk_start, chunk_end)
  old_span <- chunk_span
  
  loop <- 0
  # first retrieval
  print(glue::glue("Loop {loop}: retrieving data from {chunk_start} to {chunk_end}"))
  first_chunk <- retrieve_data(keyword, chunk_span)
  
  while (chunk_end <= end_date) {
    
    loop <- loop + 1  
  
    if (loop %% 15 == 0) {
      sleep_for <- 60 + runif(1,1,20) %>% round()
      print(glue::glue("Sleep for {sleep_for} to prevent ban"))
      Sys.sleep(sleep_for)
    }
    
    # shift time window
    chunk_start <- fetch_local_max_date(first_chunk)
    
    # if chunk_end > end_date, chunk_end = end_date
    if ( (chunk_start + window) > end_date ) {
      chunk_end <- end_date
    } else {
      chunk_end <- chunk_start + window
    }
    
    # define new span
    chunk_span <- make_timespan(chunk_start, chunk_end)
    
    # if the span does not change, add one day
    if (chunk_span == old_span) {
      chunk_start <- chunk_start + 269
      chunk_end <- chunk_end + 269
      
      chunk_span <- make_timespan(chunk_start, chunk_end)
    }
    
    # print some info
    print(glue::glue("Loop {loop}: retrieving data from {chunk_start} to {chunk_end}"))
    
    # perform another retrieval
    second_chunk <- retrieve_data(keyword, chunk_span) %>% 
      scale_series()
    
    first_chunk <- concatenate_series(first_chunk, second_chunk)
    
    old_span <- chunk_span
    
  }
  
  return(first_chunk)
  
}
```

```{r}
retrieve_daily_data(start = "2012-01-01", end = "2021-08-28", keyword="Mario Draghi")
```

